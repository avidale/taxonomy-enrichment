{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "import json\n",
    "import gensim\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def add_sys_path(p):\n",
    "    p = os.path.abspath(p)\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "\n",
    "add_sys_path('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ds = evaluate.read_dataset('../data/training_data/training_nouns.tsv',  lambda x: json.loads(x))\n",
    "train, dev, test1, test2, hid, forbidden_words = data_split.split_dict(n_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5083, 25376)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(forbidden_words), len(n_ds.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forbidden_words = set(n_ds.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_knn\n",
    "from importlib import reload\n",
    "reload(my_knn)\n",
    "from my_knn import SynsetStorage, RelationStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "taiga = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'C:/Users/ddale/Downloads/NLP/rusvectores/taiga_skipgram/model.bin', \n",
    "    binary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_embedder_pos = my_knn.W2VWrapper(taiga,  pos_weights={'NOUN': 1.0, 'PREP': 0.1}, default_weight=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_embedder = my_knn.W2VWrapper(taiga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = gensim.models.fasttext.FastTextKeyedVectors.load(\n",
    "    'C:/Users/ddale/Downloads/NLP/rusvectores/model.model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "ft_embedder = my_knn.SentenceEmbedder(ft=ft, n=300, normalize_word=True, pos_weights={'PREP': 0.1})\n",
    "print(ft_embedder('привет как дела').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of texts: 86549\n",
      "forbidden senses are 4126\n",
      "numer of ids 29296 long list is 95119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77484d9b940c422aa2f9ac632812cb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "23827\n",
      "466\n",
      "5\n",
      "30877\n"
     ]
    }
   ],
   "source": [
    "syns_storage, rel_storage, rel_df = my_knn.prepare_storages(\n",
    "    synsets_filename='../data/ruwordnet/synsets.N.xml',\n",
    "    relations_filename='../data/ruwordnet/synset_relations.N.xml',\n",
    "    forbidden_words=forbidden_words,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df439a67df0432880ee49ce03c1cda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=95119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_vecs_pos = np.stack([w2v_embedder_pos(t) for t in tqdm(syns_storage.texts_long) ])\n",
    "w2v_tree_pos = KDTree(w2v_vecs_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272727272727273"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textdistance import damerau_levenshtein\n",
    "damerau_levenshtein('лев', 'левобережье') / len('левобережье')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ranking\n",
    "reload(ranking);\n",
    "import download_wiki\n",
    "reload(download_wiki);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1000 definitions from file wiki_private_nouns_0_1000.pkl\n",
      "Got 525 definitions from file wiki_private_nouns_1000_2000.pkl\n",
      "Got 762 definitions from file wiki_public_nouns_0_1000.pkl\n",
      "Got 810 definitions from file wiki_training_nouns_0_1000.pkl\n",
      "Got 787 definitions from file wiki_training_nouns_10000_11000.pkl\n",
      "Got 803 definitions from file wiki_training_nouns_1000_2000.pkl\n",
      "Got 770 definitions from file wiki_training_nouns_11000_12000.pkl\n",
      "Got 794 definitions from file wiki_training_nouns_12000_13000.pkl\n",
      "Got 793 definitions from file wiki_training_nouns_13000_14000.pkl\n",
      "Got 798 definitions from file wiki_training_nouns_14000_15000.pkl\n",
      "Got 783 definitions from file wiki_training_nouns_15000_16000.pkl\n",
      "Got 772 definitions from file wiki_training_nouns_16000_17000.pkl\n",
      "Got 803 definitions from file wiki_training_nouns_17000_18000.pkl\n",
      "Got 732 definitions from file wiki_training_nouns_18000_19000.pkl\n",
      "Got 740 definitions from file wiki_training_nouns_19000_20000.pkl\n",
      "Got 763 definitions from file wiki_training_nouns_20000_21000.pkl\n",
      "Got 764 definitions from file wiki_training_nouns_2000_3000.pkl\n",
      "Got 756 definitions from file wiki_training_nouns_21000_22000.pkl\n",
      "Got 785 definitions from file wiki_training_nouns_22000_23000.pkl\n",
      "Got 810 definitions from file wiki_training_nouns_23000_24000.pkl\n",
      "Got 785 definitions from file wiki_training_nouns_24000_25000.pkl\n",
      "Got 802 definitions from file wiki_training_nouns_25000_26000.pkl\n",
      "Got 805 definitions from file wiki_training_nouns_26000_27000.pkl\n",
      "Got 809 definitions from file wiki_training_nouns_27000_28000.pkl\n",
      "Got 775 definitions from file wiki_training_nouns_28000_29000.pkl\n",
      "Got 756 definitions from file wiki_training_nouns_29000_30000.pkl\n",
      "Got 795 definitions from file wiki_training_nouns_30000_31000.pkl\n",
      "Got 789 definitions from file wiki_training_nouns_3000_4000.pkl\n",
      "Got 758 definitions from file wiki_training_nouns_31000_32000.pkl\n",
      "Got 380 definitions from file wiki_training_nouns_32000_33000.pkl\n",
      "Got 747 definitions from file wiki_training_nouns_4000_5000.pkl\n",
      "Got 806 definitions from file wiki_training_nouns_5000_6000.pkl\n",
      "Got 801 definitions from file wiki_training_nouns_6000_7000.pkl\n",
      "Got 805 definitions from file wiki_training_nouns_7000_8000.pkl\n",
      "Got 757 definitions from file wiki_training_nouns_8000_9000.pkl\n",
      "Got 753 definitions from file wiki_training_nouns_9000_10000.pkl\n",
      "Current cache size is 27654\n"
     ]
    }
   ],
   "source": [
    "wiki_storage = download_wiki.CachedDownloader(downloader=download_wiki.get_definition)\n",
    "wiki_storage.collect_from_files('cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_candidates(\n",
    "        text,\n",
    "        synset_storage: SynsetStorage,\n",
    "        rel_storage: RelationStorage,\n",
    "        index=None,\n",
    "        text2vec=None,\n",
    "        k=10,\n",
    "        verbose=False,\n",
    "):\n",
    "    ids_list = synset_storage.ids_long\n",
    "    texts_list = synset_storage.texts_long\n",
    "\n",
    "    vec = text2vec(text)\n",
    "    distances, indices = index.query(vec.reshape(1, -1), k=k)\n",
    "    \n",
    "    hypotheses = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    vec_plain = w2v_embedder.get_text_vec(text.lower())\n",
    "    vec_ft = ft_embedder(text.lower())\n",
    "    for i, d in zip(indices.ravel(), distances.ravel()):\n",
    "        hypers = rel_storage.id2hypernym.get(ids_list[i], set())\n",
    "        t1, t2 = text.lower(), texts_list[i].lower()\n",
    "        \n",
    "        splain = np.dot(vec_plain, w2v_embedder.get_text_vec(t2))\n",
    "        sim_ft = np.dot(vec_ft, ft_embedder(t2))\n",
    "        #leven_min = damerau_levenshtein(t1, t2) / min(len(t1), len(t2))\n",
    "        #leven_max = damerau_levenshtein(t1, t2) / max(len(t1), len(t2))\n",
    "        s = (1-d**2/2)\n",
    "        magic = np.exp(-d ** 3) * (splain ** 5)\n",
    "\n",
    "        for parent in hypers:\n",
    "            ids = [parent] + sorted(rel_storage.id2hypernym.get(parent, set()))\n",
    "            for j, hyper_id in enumerate(ids):\n",
    "                int_o = 1 + (j > 0)\n",
    "                o = str(int_o)\n",
    "                h = hypotheses[hyper_id]\n",
    "                h['query'] = text\n",
    "                h['document'] = hyper_id\n",
    "                h['neighbor'] = h.get('neighbor', texts_list[i])\n",
    "                h['sum_magic'] += magic / int_o\n",
    "                h['sum_magic_exp'] += np.exp(-d ** 3) / int_o\n",
    "                h['sum_magic_s5'] += (splain ** 5) / int_o\n",
    "                h['sum_ft_sim'] += sim_ft / int_o\n",
    "                h['sum_ft_sim_5'] += sim_ft ** 5 / int_o\n",
    "                continue\n",
    "\n",
    "                h['first_heighbor_' + o] = min(h.get('first_heighbor_' + o, 100500), i)\n",
    "                h['n_' + o] += 1\n",
    "                h['max_dist_' + o] = max(h['max_dist_' + o], d)\n",
    "                h['min_dist_' + o] = min(h.get('min_dist_' + o, 100), d)\n",
    "                #h['min_leven_min_' + o] = min(h.get('min_leven_min_' + o, 100), leven_min)\n",
    "                #h['min_leven_max_' + o] = min(h.get('min_leven_max_' + o, 100), leven_max)\n",
    "                h['sum_magic_' + o] += magic\n",
    "                h['sum_dist_' + o] += d\n",
    "                h['sum_dist2_' + o] += d**2\n",
    "                h['sum_dist3_' + o] += d**3\n",
    "                h['sum_sim_'+o] += s\n",
    "                h['sum_sim2_'+o] += s**2\n",
    "                h['sum_sim5_'+o] += s**5\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys = sorted(train.keys())\n",
    "random.seed(42)\n",
    "train_small_keys = random.sample(train_keys, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = train_small_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь тренировочный сет заваривается полчаса; ужас!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1c4675cc554dfc9b20868feb5f196b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pre_x = []\n",
    "pre_y = []\n",
    "query_ids = []\n",
    "for i, w in enumerate(tqdm(keys)):\n",
    "    cands = knn_candidates(\n",
    "        w, \n",
    "        index=w2v_tree_pos, text2vec=w2v_embedder_pos, \n",
    "        synset_storage=syns_storage, rel_storage=rel_storage,\n",
    "        k=100,\n",
    "    )\n",
    "    cands = ranking.add_defin_hypotheses(\n",
    "        w, cands, \n",
    "        definition_extractor=wiki_storage,\n",
    "        synset_storage=syns_storage,\n",
    "    )\n",
    "    gt_labels = {s for senses in train[w] for s in senses}\n",
    "    for cand_id, cand_dict in cands.items():\n",
    "        query_ids.append(i)\n",
    "        pre_y.append(cand_id in gt_labels)\n",
    "        pre_x.append(dict(cand_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(pre_y)\n",
    "\n",
    "def extract_features(pre_x, remove_meta=True):\n",
    "    x = pd.DataFrame(pre_x)\n",
    "    x.fillna(0, inplace=True)\n",
    "\n",
    "    if remove_meta:\n",
    "        for col in ['query', 'document', 'neighbor']:\n",
    "            if col in x.columns:\n",
    "                x.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    eps = 1e-12\n",
    "    if 'sum_dist_1' in x.columns and 'n_1' in x.columns:\n",
    "        x['mean_dist_1'] = x.sum_dist_1 / (x.n_1 + eps)\n",
    "        x['mean_dist_2'] = x.sum_dist_2 / (x.n_2 + eps)\n",
    "        x['mean_dist_all'] = (x.sum_dist_1 + x.sum_dist_2) / (x.n_1 + x.n_2 + eps)\n",
    "        x['has_1'] = (x.n_1 > 0).astype(int)\n",
    "        x['has_2'] = (x.n_2 > 0).astype(int)\n",
    "    if 'wiki_n_matches' in x.columns:\n",
    "        x['wiki_has_matches'] = (x['wiki_n_matches'] > 0).astype(int)\n",
    "    return x\n",
    "\n",
    "x = extract_features(pre_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Только половина запросов вообще хоть как-то нашлась в вики. грустно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.363"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = extract_features(pre_x, remove_meta=False)\n",
    "xx.groupby('query').wiki_has_matches.max().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_magic</th>\n",
       "      <th>sum_magic_exp</th>\n",
       "      <th>sum_magic_s5</th>\n",
       "      <th>sum_ft_sim</th>\n",
       "      <th>sum_ft_sim_5</th>\n",
       "      <th>wiki_min_place</th>\n",
       "      <th>wiki_match_len</th>\n",
       "      <th>wiki_n_senses</th>\n",
       "      <th>wiki_n_matches</th>\n",
       "      <th>wiki_has_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99073</th>\n",
       "      <td>0.007764</td>\n",
       "      <td>0.336388</td>\n",
       "      <td>0.023081</td>\n",
       "      <td>0.290825</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47993</th>\n",
       "      <td>0.013458</td>\n",
       "      <td>0.381565</td>\n",
       "      <td>0.035271</td>\n",
       "      <td>0.362385</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34460</th>\n",
       "      <td>0.021929</td>\n",
       "      <td>0.773390</td>\n",
       "      <td>0.053350</td>\n",
       "      <td>0.657257</td>\n",
       "      <td>0.067686</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sum_magic  sum_magic_exp  sum_magic_s5  sum_ft_sim  sum_ft_sim_5  \\\n",
       "99073   0.007764       0.336388      0.023081    0.290825      0.002080   \n",
       "47993   0.013458       0.381565      0.035271    0.362385      0.006250   \n",
       "34460   0.021929       0.773390      0.053350    0.657257      0.067686   \n",
       "\n",
       "       wiki_min_place  wiki_match_len  wiki_n_senses  wiki_n_matches  \\\n",
       "99073               0               0              0               0   \n",
       "47993               0               0              0               0   \n",
       "34460               0               0              0               0   \n",
       "\n",
       "       wiki_has_matches  \n",
       "99073                 0  \n",
       "47993                 0  \n",
       "34460                 0  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_magic</th>\n",
       "      <th>sum_magic_exp</th>\n",
       "      <th>sum_magic_s5</th>\n",
       "      <th>sum_ft_sim</th>\n",
       "      <th>sum_ft_sim_5</th>\n",
       "      <th>wiki_min_place</th>\n",
       "      <th>wiki_match_len</th>\n",
       "      <th>wiki_n_senses</th>\n",
       "      <th>wiki_n_matches</th>\n",
       "      <th>wiki_has_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99862.000000</td>\n",
       "      <td>99862.000000</td>\n",
       "      <td>99862.000000</td>\n",
       "      <td>99862.000000</td>\n",
       "      <td>99862.000000</td>\n",
       "      <td>99862.000000</td>\n",
       "      <td>99862.000000</td>\n",
       "      <td>99862.000000</td>\n",
       "      <td>99862.000000</td>\n",
       "      <td>99862.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.147855</td>\n",
       "      <td>1.099073</td>\n",
       "      <td>0.210207</td>\n",
       "      <td>1.152739</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>11.008752</td>\n",
       "      <td>0.759158</td>\n",
       "      <td>0.368398</td>\n",
       "      <td>0.141806</td>\n",
       "      <td>0.138782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.621520</td>\n",
       "      <td>2.626276</td>\n",
       "      <td>0.742241</td>\n",
       "      <td>2.655267</td>\n",
       "      <td>0.847020</td>\n",
       "      <td>39.406292</td>\n",
       "      <td>2.333717</td>\n",
       "      <td>1.077094</td>\n",
       "      <td>0.358172</td>\n",
       "      <td>0.345720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.292165</td>\n",
       "      <td>-0.002129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176272</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.409329</td>\n",
       "      <td>0.034689</td>\n",
       "      <td>0.455378</td>\n",
       "      <td>0.029244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.076041</td>\n",
       "      <td>1.001826</td>\n",
       "      <td>0.138971</td>\n",
       "      <td>1.143666</td>\n",
       "      <td>0.157405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>36.411295</td>\n",
       "      <td>86.606781</td>\n",
       "      <td>37.283963</td>\n",
       "      <td>77.945470</td>\n",
       "      <td>39.291219</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sum_magic  sum_magic_exp  sum_magic_s5    sum_ft_sim  sum_ft_sim_5  \\\n",
       "count  99862.000000   99862.000000  99862.000000  99862.000000  99862.000000   \n",
       "mean       0.147855       1.099073      0.210207      1.152739      0.228900   \n",
       "std        0.621520       2.626276      0.742241      2.655267      0.847020   \n",
       "min        0.000000       0.000000      0.000000     -0.292165     -0.002129   \n",
       "25%        0.000000       0.203055      0.000000      0.176272      0.000693   \n",
       "50%        0.014535       0.409329      0.034689      0.455378      0.029244   \n",
       "75%        0.076041       1.001826      0.138971      1.143666      0.157405   \n",
       "max       36.411295      86.606781     37.283963     77.945470     39.291219   \n",
       "\n",
       "       wiki_min_place  wiki_match_len  wiki_n_senses  wiki_n_matches  \\\n",
       "count    99862.000000    99862.000000   99862.000000    99862.000000   \n",
       "mean        11.008752        0.759158       0.368398        0.141806   \n",
       "std         39.406292        2.333717       1.077094        0.358172   \n",
       "min          0.000000        0.000000       0.000000        0.000000   \n",
       "25%          0.000000        0.000000       0.000000        0.000000   \n",
       "50%          0.000000        0.000000       0.000000        0.000000   \n",
       "75%          0.000000        0.000000       0.000000        0.000000   \n",
       "max        625.000000       47.000000       9.000000        4.000000   \n",
       "\n",
       "       wiki_has_matches  \n",
       "count      99862.000000  \n",
       "mean           0.138782  \n",
       "std            0.345720  \n",
       "min            0.000000  \n",
       "25%            0.000000  \n",
       "50%            0.000000  \n",
       "75%            0.000000  \n",
       "max            1.000000  "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for w in train_small_keys:\n",
    "    if w in syns_storage.word2sense:\n",
    "        print(w)\n",
    "        print(train[w])\n",
    "        print(syns_storage.word2sense[w])\n",
    "        print([rel_storage.id2hypernym[s] for s in syns_storage.word2sense[w]])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99862, 10) 2696\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, sum(pre_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94755809 0.93446388 0.94661316]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000, C=0.01, penalty='l1', solver='liblinear')\n",
    "#model = CatBoostClassifier(iterations=100, depth=3, verbose=0)\n",
    "print(cross_val_score(model, x, y, cv=3, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.88538203 0.8336317  0.85868983]  100 words\n",
    "[0.85867767 0.87508228 0.85045668]  300 words\n",
    "[0.86614533 0.85163482 0.85033878] 1000 words\n",
    "[0.89062933 0.83815102 0.87119477]  100 words, 13 features\n",
    "[0.95877762 0.93675879 0.92065066]  100 words, 15 features (with levenshtein)\n",
    "[0.74918301 0.86308356 0.81610435]  100 words, 13 features, drop all train words\n",
    "[0.95552977 0.93510626 0.92946495]  100 words, 23 features (with similarity)\n",
    "[0.94820527 0.94484599 0.95262975] 3000 words, 23 features\n",
    "[0.94824586 0.95184857 0.95281711]  all words, 23 features\n",
    "[0.95035338 0.9292177  0.90908593]  100 words, magic-only\n",
    "[0.95491309 0.95448801 0.93524542]  100 words, magic-only with uniform-POS scorer\n",
    "[0.95754426 0.95235692 0.92833908]  100 words, magic-only with uniform-POS scorer and fixed power\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum_magic           2.669586\n",
       "sum_magic_exp      -1.057455\n",
       "sum_magic_s5        0.000000\n",
       "sum_ft_sim          0.873499\n",
       "sum_ft_sim_5        0.000000\n",
       "wiki_min_place     -0.011819\n",
       "wiki_match_len      0.071185\n",
       "wiki_n_senses      -0.016117\n",
       "wiki_n_matches      0.000000\n",
       "wiki_has_matches    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y);\n",
    "pd.Series(model.coef_[0], index=x.columns)\n",
    "#pd.Series(model.feature_importances_, index=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d39848d954a4bbea593c9e2cf3b782a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pre_features_dict = dict()\n",
    "labels_dict = dict()\n",
    "for i, w in enumerate(tqdm(dev)):\n",
    "    cands = knn_candidates(\n",
    "        w, \n",
    "        index=w2v_tree_pos, text2vec=w2v_embedder_pos, \n",
    "        synset_storage=syns_storage, rel_storage=rel_storage,\n",
    "        k=100,\n",
    "    )\n",
    "    cands = ranking.add_defin_hypotheses(\n",
    "        w, cands, \n",
    "        definition_extractor=wiki_storage,\n",
    "        synset_storage=syns_storage,\n",
    "    )\n",
    "    labels = []\n",
    "    pre_features = []\n",
    "    for cand_id, cand_dict in cands.items():\n",
    "        labels.append(cand_id)\n",
    "        pre_features.append(dict(cand_dict))\n",
    "    pre_features_dict[w] = pre_features\n",
    "    labels_dict[w] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1045f8a12ddb435caded182e9831ee74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = dict()\n",
    "\n",
    "for i, w in enumerate(tqdm(dev)):\n",
    "    features = extract_features(pre_features_dict[w])\n",
    "    scores = model.predict_proba(features)[:, 1]\n",
    "    predictions[w] = pd.Series(scores, index=labels_dict[w]).sort_values(ascending=False).head(10).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4522257634462358 0.4832161604799399\n"
     ]
    }
   ],
   "source": [
    "mean_ap, mean_rr = evaluate.get_score(dev, predictions, k=10)\n",
    "print(mean_ap, mean_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0.4695603804732742 0.507339707536557  * previous baseline\n",
    "0.3971516581260675 0.42608580177477784  c=1    , три магические фичи\n",
    "0.4195125348914717 0.44960629921259826  c=0.1\n",
    "0.4663870141232345 0.50311914135733     c=0.01\n",
    "0.4657120984876891 0.5006608548931382   with wikipedia data ='(\n",
    "0.4442544942298880 0.4749015748031495  with wiki data and catboost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0.4695603804732742 0.507339707536557 * - previous baseline\n",
    "0.440481918926801  0.4710614298212723  - logreg scorer (100 training samples, 9 features)\n",
    "0.4466926660209143 0.4790729283839521  - 300 samples\n",
    "0.4468210353914096 0.4791447944007000  - 1000 samples\n",
    "0.4582034667541558 0.4897161292338458+ - 100 samples, 13 features\n",
    "0.4394665510561180 0.4742040057492813  - 100 samples, 15 features, looks like overfit\n",
    "0.3241014144065326 0.3557024121984751  - 100 samples, 13 features, with more forbidden words\n",
    "0.4306767643627878 0.4671080177477813  - 100 samples, 23 features, with similarity\n",
    "0.4626613079615048 0.4990266841644794  - 3k samples, 23 features; almost there!\n",
    "0.4670162323459567 0.5041869766279214  - all samples, 23 features - still not there, hmmm...\n",
    "0.4584119693371660 0.4955021247344079  - 100 samples + magic + fixed similarities \n",
    "0.4584119693371660 0.4955021247344079  - magic as a single feature; why are we still not here???\n",
    "0.4395115975086445 0.4733978565179351  - magic (uni-pos w2v) as a single feature; WTF???\n",
    "0.4691010628879724 0.5070444319460065  - fixed a typo in magic and funally reproduced the baseline\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### смотрим, что же не так с ответами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "не найдены\n",
    "* named entities (ФСТР)\n",
    "* абстракции (гладкость, въедливость, состязательное судопроизводство)\n",
    "\n",
    "неверные соседи\n",
    "* фразы (диагноз болезни, восточная европа, смещение на низшую должность)\n",
    "\n",
    "много значений\n",
    "* гео (казанка, босанцы, туниска)\n",
    "* честно многозначное (рубка)\n",
    "\n",
    "абстрактные гиперонимы\n",
    "* пенек\n",
    "\n",
    "плохо отличаю верхнеуровневые сущности (процессы от предметов от свойств и т.п.)\n",
    "* крошение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "СОСТЯЗАТЕЛЬНОЕ СУДОПРОИЗВОДСТВО\n"
     ]
    }
   ],
   "source": [
    "w = np.random.choice(list(dev.keys()))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_set = {t for tt in dev[w] for t in tt}\n",
    "cand_set = {c['document'] for c in pre_features_dict[w]}\n",
    "res_set = predictions[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "106454-N 0 0 ПОЛОЖЕНИЕ, ТЕЗИС\n",
      "106825-N 0 0 ПРИНЦИП, ОСНОВНОЕ ПОЛОЖЕНИЕ\n"
     ]
    }
   ],
   "source": [
    "for i, concomp in enumerate(dev[w]):\n",
    "    print(i)\n",
    "    for synset_id in concomp:\n",
    "        print(\n",
    "            synset_id, \n",
    "            int(synset_id in predictions[w]), \n",
    "            int(synset_id in cand_set), \n",
    "            syns_storage.get_synset_name(synset_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106562-N 0 СФЕРА ДЕЯТЕЛЬНОСТИ\n",
      "106898-N 0 РАССМОТРЕТЬ, РАЗОБРАТЬ\n",
      "149658-N 0 СЛУШАНИЕ (ОБСУЖДЕНИЕ)\n",
      "106501-N 0 ЗАНЯТИЕ, ДЕЯТЕЛЬНОСТЬ\n",
      "134994-N 0 СУДЕБНЫЙ ДОКУМЕНТ\n",
      "3199-N 0 СУДЕБНОЕ РЕШЕНИЕ\n",
      "1853-N 0 СУДОПРОИЗВОДСТВО\n",
      "115790-N 0 СИСТЕМА (ОРГАНИЗОВАННОЕ ЦЕЛОЕ)\n",
      "143739-N 0 РАЗБИРАТЕЛЬСТВО СПОРА\n",
      "661-N 0 СУД\n"
     ]
    }
   ],
   "source": [
    "for synset_id in predictions[w]:\n",
    "    print(\n",
    "        synset_id,\n",
    "        int(synset_id in gt_set),\n",
    "        syns_storage.get_synset_name(synset_id),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
