{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут находится мой бейзлайн, адаптированный для английского языка. \n",
    "\n",
    "Как запустить:\n",
    "1. установить requirements.txt\n",
    "2. скачать английский wordnet и положить в папку data/wordnet_data\n",
    "3. возможно, скачать какой-то ваш любимый английский word2vec и указать правильный путь до него в ячейках в следующем разделе\n",
    "4. параметрами кодирования текстов (например, весами POS и добавлением POS тегов при поиске слов) можно управлять в ячейках, где создаётся W2VWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import WordNetCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dale/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/dale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dale/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import lru_cache\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_taxonomy = WordNetCorpusReader(os.path.join('data', 'WN2.0'), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3129, 2)\n"
     ]
    }
   ],
   "source": [
    "nouns_data = pd.read_csv(os.path.join('data', 'wordnet_data', 'nouns_en_new.2.0-3.0.tsv'), sep='\\t', header=None)\n",
    "nouns_data.columns = ['word', 'hypernyms']\n",
    "nouns_data['hypernyms'] = nouns_data['hypernyms'].apply(json.loads)\n",
    "print(nouns_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>hypernyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>Greco-Roman_wrestling</td>\n",
       "      <td>[wrestling.n.02, contact_sport.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>Rastafarian</td>\n",
       "      <td>[disciple.n.01, follower.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>core_memory</td>\n",
       "      <td>[random-access_memory.n.01, volatile_storage.n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>Succos</td>\n",
       "      <td>[festival.n.01, religious_festival.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>neoconservatism</td>\n",
       "      <td>[conservatism.n.01, political_orientation.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>Raja_batis</td>\n",
       "      <td>[ray.n.07, skate.n.02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>coronary_heart_disease</td>\n",
       "      <td>[cardiovascular_disease.n.01, heart_disease.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>Petersburg</td>\n",
       "      <td>[operation.n.05, campaign.n.03]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>historiography</td>\n",
       "      <td>[writing.n.02, literature.n.03]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>rawness</td>\n",
       "      <td>[ignorance.n.01, content.n.05]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        word  \\\n",
       "969    Greco-Roman_wrestling   \n",
       "2901             Rastafarian   \n",
       "772              core_memory   \n",
       "1735                  Succos   \n",
       "1089         neoconservatism   \n",
       "1563              Raja_batis   \n",
       "1662  coronary_heart_disease   \n",
       "279               Petersburg   \n",
       "616           historiography   \n",
       "2975                 rawness   \n",
       "\n",
       "                                              hypernyms  \n",
       "969                [wrestling.n.02, contact_sport.n.01]  \n",
       "2901                     [disciple.n.01, follower.n.01]  \n",
       "772   [random-access_memory.n.01, volatile_storage.n...  \n",
       "1735           [festival.n.01, religious_festival.n.01]  \n",
       "1089    [conservatism.n.01, political_orientation.n.01]  \n",
       "1563                             [ray.n.07, skate.n.02]  \n",
       "1662  [cardiovascular_disease.n.01, heart_disease.n.01]  \n",
       "279                     [operation.n.05, campaign.n.03]  \n",
       "616                     [writing.n.02, literature.n.03]  \n",
       "2975                     [ignorance.n.01, content.n.05]  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(207, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>hypernyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>pumice</td>\n",
       "      <td>[guide.v.05, rub.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>currycomb</td>\n",
       "      <td>[comb.v.01, straighten.v.02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>degrease</td>\n",
       "      <td>[get_rid_of.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>cutinize</td>\n",
       "      <td>[change.v.01, convert.v.02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>nol.pros.</td>\n",
       "      <td>[discontinue.v.01, drop.v.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>devein</td>\n",
       "      <td>[get_rid_of.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>air-ship</td>\n",
       "      <td>[transport.v.01, freight.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>sideline</td>\n",
       "      <td>[demote.v.01, delegate.v.02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>metalize</td>\n",
       "      <td>[cover.v.01, coat.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>yak</td>\n",
       "      <td>[communicate.v.02, interact.v.01]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word                          hypernyms\n",
       "125     pumice             [guide.v.05, rub.v.01]\n",
       "51   currycomb       [comb.v.01, straighten.v.02]\n",
       "153   degrease                  [get_rid_of.v.01]\n",
       "205   cutinize        [change.v.01, convert.v.02]\n",
       "83   nol.pros.      [discontinue.v.01, drop.v.07]\n",
       "42      devein                  [get_rid_of.v.01]\n",
       "18    air-ship     [transport.v.01, freight.v.01]\n",
       "183   sideline       [demote.v.01, delegate.v.02]\n",
       "174   metalize            [cover.v.01, coat.v.01]\n",
       "114        yak  [communicate.v.02, interact.v.01]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_data = pd.read_csv(os.path.join('data', 'wordnet_data', 'verbs_en_new.2.0-3.0.tsv'), sep='\\t', header=None)\n",
    "verbs_data.columns = ['word', 'hypernyms']\n",
    "verbs_data['hypernyms'] = verbs_data['hypernyms'].apply(json.loads)\n",
    "print(verbs_data.shape)\n",
    "verbs_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate # from https://github.com/dialogue-evaluation/taxonomy-enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VB'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_parse(\"get\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air-ship']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('air-ship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-02 15:50:18--  http://vectors.nlpl.eu/repository/20/29.zip\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 667161074 (636M) [application/zip]\n",
      "Saving to: ‘29.zip’\n",
      "\n",
      "29.zip              100%[===================>] 636,25M  20,3MB/s    in 2m 13s  \n",
      "\n",
      "2020-11-02 15:52:31 (4,80 MB/s) - ‘29.zip’ saved [667161074/667161074]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://vectors.nlpl.eu/repository/20/29.zip  # http://vectors.nlpl.eu/explore/embeddings/en/models/ gigaword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  29.zip\n",
      "  inflating: gigaword/meta.json      \n",
      "  inflating: gigaword/model.bin      \n",
      "  inflating: gigaword/model.txt      \n",
      "  inflating: gigaword/README         \n"
     ]
    }
   ],
   "source": [
    "!unzip 29.zip -d gigaword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_vecs = gensim.models.KeyedVectors.load_word2vec_format('gigaword/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mispronounce_VERB', 0.4861705005168915),\n",
       " ('type_VERB', 0.4858720600605011),\n",
       " ('typing_NOUN', 0.4777812957763672),\n",
       " ('bookmark_VERB', 0.47726932168006897),\n",
       " ('spell-check_NOUN', 0.4647689461708069),\n",
       " ('facebook_NOUN', 0.46326524019241333),\n",
       " ('retype_VERB', 0.4609655737876892),\n",
       " ('username_ADJ', 0.45739173889160156),\n",
       " ('mistype_VERB', 0.452288419008255),\n",
       " ('login_NOUN', 0.4498198628425598)]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw_vecs.most_similar('google_VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_vecs = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=50_000)\n",
    "def lemmatize(word):\n",
    "    return lemmatizer.lemmatize(word) or word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=50_000)\n",
    "def morph_parse(w):\n",
    "    return nltk.pos_tag([w])[0][1]\n",
    "\n",
    "tokenize = nltk.word_tokenize\n",
    "word2pos  = morph_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v, epsilon=1e-10):\n",
    "    return v / (sum(v**2)**0.5 + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSentenceEmbedder:\n",
    "    def __init__(self, n=300, normalize_word=True, pos_weights=None, default_weight=1):\n",
    "        self.n = n\n",
    "        self.normalize_word = normalize_word\n",
    "        self.pos_weights = pos_weights\n",
    "        self.default_weight = default_weight\n",
    "\n",
    "    def get_word_vec(self, word):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @lru_cache(maxsize=1024)\n",
    "    def __call__(self, text):\n",
    "        tokens = tokenize(text)\n",
    "        weights = self.get_word_weights(tokens)\n",
    "        vecs = [self.get_word_vec(word) for word in tokens]\n",
    "        if self.normalize_word:\n",
    "            vecs = [normalize(v) for v in vecs]\n",
    "        if len(vecs) == 0:\n",
    "            return np.zeros(self.n)\n",
    "        return normalize(sum([vec * weight for vec, weight in zip(vecs, weights)]))\n",
    "\n",
    "    def get_word_weights(self, tokens):\n",
    "        if not self.pos_weights:\n",
    "            return [1] * len(tokens)\n",
    "        return [self.pos_weights.get(word2pos(t), self.default_weight) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VWrapper(BaseSentenceEmbedder):\n",
    "    POS_MAP = {\n",
    "        'INFN': 'VERB',\n",
    "        'ADJF': 'ADJ',\n",
    "        'ADVB': 'ADV',\n",
    "    }\n",
    "    POS_MISS = {\n",
    "        'PREP'\n",
    "    }\n",
    "\n",
    "    def __init__(self, w2v, morph=morph_parse, append_pos=False, **kwargs):\n",
    "        super(W2VWrapper, self).__init__(**kwargs)\n",
    "        self.w2v = w2v\n",
    "        self.morph = morph\n",
    "        self.prefix2word = self.make_prefixes()\n",
    "        self.append_pos = append_pos\n",
    "\n",
    "    def make_prefixes(self):\n",
    "        prefix2word = defaultdict(set)\n",
    "        for w in tqdm(self.w2v.vocab.keys()):\n",
    "            for n in range(1, len(w) - 2):\n",
    "                prefix2word[w[:-n]].add(w)\n",
    "        prefix2word = {k: v for k, v in prefix2word.items()}\n",
    "        return prefix2word\n",
    "\n",
    "    def find_prefix(self, word, min_len=2):\n",
    "        mapping = self.prefix2word\n",
    "        if word in mapping:\n",
    "            return mapping[word]\n",
    "        for i in range(1, len(word) - min_len):\n",
    "            t = mapping.get(word[:-i])\n",
    "            if t is not None:\n",
    "                return t\n",
    "        return None\n",
    "\n",
    "    def get_text_vec(self, text, verbose=False):\n",
    "        toks = tokenize(text)\n",
    "        vecs = []\n",
    "        for tok in toks:\n",
    "            vecs.append(self.get_word_vec(word=tok, verbose=verbose))\n",
    "        if not vecs:\n",
    "            return np.zeros(self.w2v.vectors.shape[1])\n",
    "        return normalize(sum(vecs))\n",
    "\n",
    "    def add_pos(self, word):\n",
    "        tag = self.morph(word)\n",
    "        if not tag:\n",
    "            return word\n",
    "        new_pos = self.POS_MAP.get(tag, tag or '-')\n",
    "        return word + '_' + new_pos\n",
    "    \n",
    "    def get_word_vec(self, word, verbose=False, add_pos=False):\n",
    "        key = word\n",
    "        key2 = lemmatize(word)\n",
    "        if self.append_pos:\n",
    "            key = self.add_pos(key)\n",
    "            key2 = self.add_pos(key2)\n",
    "        if verbose:\n",
    "            print(key, key2, self.find_prefix(key))\n",
    "        if key in self.w2v.vocab:\n",
    "            return self.w2v[key]\n",
    "        elif key2 in self.w2v.vocab:\n",
    "            return self.w2v[key2]\n",
    "        else:\n",
    "            keys = self.find_prefix(key)\n",
    "            if keys:\n",
    "                return sum([self.w2v[k] for k in keys]) / len(keys)\n",
    "        return np.zeros(self.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb46cd5b60b4a84aa3a547948aa3ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=297790.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embedder_gw = W2VWrapper(gw_vecs, append_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_MAP = {\n",
    "    #CC coordinating conjunction\n",
    "    'CD': 'NUM', # cardinal digit\n",
    "    #DT determiner\n",
    "    #EX existential there (like: “there is” … think of it like “there exists”)\n",
    "    'FW': 'X', # foreign word\n",
    "    #IN preposition/subordinating conjunction\n",
    "    'JJ': 'ADJ', #adjective ‘big’\n",
    "    'JJR': 'ADJ', #adjective, comparative ‘bigger’\n",
    "    'JJS': 'ADJ', #adjective, superlative ‘biggest’\n",
    "    'LS': 'NUM', #list marker 1)\n",
    "    'MD': 'VERB',# modal could, will\n",
    "    'NN': 'NOUN', #noun, singular ‘desk’\n",
    "    'NNS': 'NOUN', #noun plural ‘desks’\n",
    "    'NNP': 'PROPN', #proper noun, singular ‘Harrison’\n",
    "    'NNPS': 'PROPN', #proper noun, plural ‘Americans’\n",
    "    #PDT predeterminer ‘all the kids’\n",
    "    'POS': 'NOUN', #possessive ending parent’s\n",
    "    #'PRP personal pronoun I, he, she\n",
    "    # PRP$ possessive pronoun my, his, hers\n",
    "    'RB': 'ADV', #adverb very, silently,\n",
    "    'RBR': 'ADV', #adverb, comparative better\n",
    "    'RBS': 'ADV', #adverb, superlative best\n",
    "    #RP particle give up\n",
    "    #TO, to go ‘to’ the store.\n",
    "    #UH interjection, errrrrrrrm\n",
    "    'VB': 'VERB', #verb, base form take\n",
    "    'VBD': 'VERB',# verb, past tense, took\n",
    "    'VBG': 'VERB', #verb, gerund/present participle taking\n",
    "    'VBN': 'VERB', #verb, past participle is taken\n",
    "    'VBP': 'VERB', #verb, sing. present, known-3d take\n",
    "    'VBZ': 'VERB', #verb, 3rd person sing. present takes\n",
    "    #WDT wh-determiner which\n",
    "    #WP wh-pronoun who, what\n",
    "    #WP$ possessive wh-pronoun whose\n",
    "    #WRB wh-adverb where, when\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_embedder_gw.POS_MAP = POS_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa49bc33dc84561a0773ae3ba6f2299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=297790.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f743264a3e2048efa49b43a3fccde687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=297790.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embedder_gw_pos_n = W2VWrapper(gw_vecs,  pos_weights={'NN': 1.0, 'NNS': 1, 'NNP': 1, 'NNPS':1, 'IN': 0.1, 'CC': 0.1}, default_weight=0.5)\n",
    "w2v_embedder_gw_pos_v = W2VWrapper(gw_vecs,  pos_weights={'VB': 1.0, 'VBG': 1, 'IN': 0.1, 'CC': 0.1} , default_weight=0.5)\n",
    "w2v_embedder_gw_pos_n.POS_MAP = POS_MAP\n",
    "w2v_embedder_gw_pos_v.POS_MAP = POS_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b2b8ac3aff45c18465568f67c0ebd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embedder = W2VWrapper(google_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b222d5172e2046c0aabc2fe7ce057d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakSet.__init__.<locals>._remove at 0x7ff478044440>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dale/anaconda3/lib/python3.7/_weakrefset.py\", line 38, in _remove\n",
      "    def _remove(item, selfref=ref(self)):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embedder_pos_n = W2VWrapper(google_vecs,  pos_weights={'NN': 1.0, 'IN': 0.1, 'CC': 0.1}, default_weight=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3291170c900849b88295559156365c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embedder_pos_v = W2VWrapper(google_vecs,  pos_weights={'VB': 1.0, 'IN': 0.1, 'CC': 0.1}, default_weight=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynsetStorage:\n",
    "    def __init__(self, id2synset, ids, ids_long, texts_long, word2sense, forbidden_id=None):\n",
    "        self.id2synset = id2synset\n",
    "        self.ids = ids\n",
    "        self.ids_long = ids_long\n",
    "        self.texts_long = texts_long\n",
    "        self.word2sense = word2sense\n",
    "        self.forbidden_id = forbidden_id or set()\n",
    "\n",
    " \n",
    "    @classmethod\n",
    "    def from_taxonomy(cls, tx, pos, forbidden_words=None):\n",
    "        id2synset = {v.name(): v for v in tx.all_synsets(pos=pos)}\n",
    "        print('synsets: {}'.format(len(id2synset)))\n",
    "        \n",
    "        word2sense = defaultdict(set)\n",
    "        for synset_id, synset in id2synset.items():\n",
    "            texts = {l.name() for l in synset.lemmas()}\n",
    "            texts.add(synset.name().split('.')[0])\n",
    "            # texts.add(synset.definition())\n",
    "            for text in texts:\n",
    "                word2sense[text].add(synset_id)\n",
    "        print('senses', len(word2sense))\n",
    "        \n",
    "        forbidden_id = set()\n",
    "        for word in (forbidden_words or {}):\n",
    "            if word in word2sense:\n",
    "                for sense_id in word2sense[word]:\n",
    "                    forbidden_id.add(sense_id)\n",
    "        print('forbidden senses are', len(forbidden_id))\n",
    "        \n",
    "        ids = sorted(id2synset.keys())\n",
    "        ids_long = []\n",
    "        texts_long = []\n",
    "        for id in tqdm(ids):\n",
    "            synset = id2synset[id]\n",
    "\n",
    "            texts = {l.name() for l in synset.lemmas()}\n",
    "            texts.add(synset.name().split('.')[0])\n",
    "            texts.add(synset.definition())\n",
    "\n",
    "            # исключаем все слова, омонимичные с тем, что есть в тестовой выборке\n",
    "            senses = {synset_id for w in texts for synset_id in word2sense[w]}\n",
    "            if senses.intersection(forbidden_id):\n",
    "                continue\n",
    "\n",
    "            if len(texts) > 1:\n",
    "                texts.add(' ; '.join(sorted(texts)))\n",
    "            for text in sorted(texts):\n",
    "                ids_long.append(id)\n",
    "                texts_long.append(text)\n",
    "        \n",
    "        print('numer of ids', len(ids), 'long list is', len(ids_long))\n",
    "        return cls(\n",
    "            id2synset=id2synset,\n",
    "            ids=ids,\n",
    "            ids_long=ids_long,\n",
    "            texts_long=texts_long,\n",
    "            word2sense=word2sense,\n",
    "            forbidden_id=forbidden_id,\n",
    "        )\n",
    "\n",
    "    def get_synset_name(self, synset_id):\n",
    "        return self.id2synset.get(synset_id, {}).get('@ruthes_name', '-')\n",
    "\n",
    "\n",
    "def make_rel_df(rel_n_raw, id2synset):\n",
    "    rel_df = pd.DataFrame(rel_n_raw['relations']['relation'])\n",
    "    rel_df['parent'] = rel_df['@parent_id'].apply(lambda x: id2synset[x]['@ruthes_name'])\n",
    "    rel_df['child'] = rel_df['@child_id'].apply(lambda x: id2synset.get(x, {}).get('@ruthes_name'))\n",
    "    return rel_df\n",
    "\n",
    "\n",
    "class RelationStorage:\n",
    "    def __init__(self, forbidden_id=None):\n",
    "        self.id2hyponym = defaultdict(set)\n",
    "        self.id2hypernym = defaultdict(set)\n",
    "        self.forbidden_id = forbidden_id or set()  # forbidden_id = set(ttest.SYNSET_ID)\n",
    "\n",
    "    def add_pair(self, hypo_id, hyper_id, max_depth=100500):\n",
    "        if max_depth <= 0:\n",
    "            return\n",
    "        if hypo_id in self.id2hyponym[hyper_id]:\n",
    "            # the pair is already here\n",
    "            return\n",
    "        if hypo_id in self.id2hypernym[hyper_id]:\n",
    "            raise ValueError('{} is already a hypernym of {}, so it cannot become its hyponym'.format(hypo_id, hyper_id))\n",
    "        for next_hypo in self.id2hyponym[hypo_id]:\n",
    "            self.add_pair(next_hypo, hyper_id, max_depth=max_depth-1)\n",
    "        for next_hyper in self.id2hypernym[hyper_id]:\n",
    "            self.add_pair(hypo_id, next_hyper, max_depth=max_depth-1)\n",
    "        self.id2hyponym[hyper_id].add(hypo_id)\n",
    "        self.id2hypernym[hypo_id].add(hyper_id)\n",
    "\n",
    "    def load_relations_from_taxonomy(self, tx, pos):\n",
    "        self.id2hyponym = defaultdict(set)\n",
    "        self.id2hypernym = defaultdict(set)\n",
    "        for v in tx.all_synsets(pos=pos):\n",
    "            hypo_id = v.name()\n",
    "            if hypo_id in self.forbidden_id:\n",
    "                continue\n",
    "            for hyper in v.hypernyms():\n",
    "                hyper_id = hyper.name()\n",
    "                if hyper_id not in self.forbidden_id:\n",
    "                    self.add_pair(hypo_id, hyper_id, max_depth=1)\n",
    "        print(len(self.id2hyponym))\n",
    "        print(max(len(c) for c in self.id2hyponym.values()))\n",
    "        print(max(len(c) for c in self.id2hypernym.values()))\n",
    "        print(sum(len(c) for c in self.id2hypernym.values()))\n",
    "            \n",
    "        \n",
    "    def construct_relations(self, rel_df):\n",
    "        self.id2hyponym = defaultdict(set)\n",
    "        self.id2hypernym = defaultdict(set)\n",
    "\n",
    "        hypo_df = rel_df[rel_df['@name'] == 'hyponym']\n",
    "        for r, row in tqdm(hypo_df.iterrows()):\n",
    "            hypo_id = row['@child_id']\n",
    "            hyper_id = row['@parent_id']\n",
    "            if hypo_id not in self.forbidden_id and hyper_id not in self.forbidden_id:\n",
    "                self.add_pair(hypo_id, hyper_id, max_depth=1)  # во второй версии поставим максимальную глубину, равную 2\n",
    "\n",
    "        print(len(self.id2hyponym))\n",
    "        print(max(len(c) for c in self.id2hyponym.values()))\n",
    "        print(max(len(c) for c in self.id2hypernym.values()))\n",
    "        print(sum(len(c) for c in self.id2hypernym.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypotheses_knn(\n",
    "        text,\n",
    "        synset_storage: SynsetStorage,\n",
    "        rel_storage: RelationStorage,\n",
    "        index=None,\n",
    "        text2vec=None,\n",
    "        k=10,\n",
    "        verbose=False,\n",
    "        decay=0,\n",
    "        grand_mult=1,\n",
    "        result_size=10,\n",
    "        return_hypotheses=False,\n",
    "        neighbor_scorer=None,\n",
    "        indexer=None,\n",
    "        scorer_pow=None,\n",
    "):\n",
    "    ids_list = synset_storage.ids_long\n",
    "    texts_list = synset_storage.texts_long\n",
    "    # todo: distance decay\n",
    "    if indexer:\n",
    "        distances, indices = indexer.query(text, k=k)\n",
    "    else:\n",
    "        vec = text2vec(text)\n",
    "        distances, indices = index.query(vec.reshape(1, -1), k=k)\n",
    "    hypotheses = Counter()\n",
    "    for i, d in zip(indices.ravel(), distances.ravel()):\n",
    "        s = 1 - d**2 / 2\n",
    "        hypers = rel_storage.id2hypernym.get(ids_list[i], set())\n",
    "\n",
    "        if neighbor_scorer is not None:\n",
    "            neighbor_score = neighbor_scorer(text, texts_list[i])\n",
    "        elif scorer_pow is not None:\n",
    "            neighbor_score = s**scorer_pow\n",
    "        else:\n",
    "            neighbor_score = 1\n",
    "        base_score = np.exp(-d ** decay) * neighbor_score\n",
    "        if verbose:\n",
    "            print(d, 1, ids_list[i], texts_list[i], len(hypers), np.exp(-d ** decay),  base_score)\n",
    "        for parent in hypers:\n",
    "            hypotheses[parent] += base_score\n",
    "            for grandparent in rel_storage.id2hypernym.get(parent, set()):\n",
    "                hypotheses[grandparent] += base_score * grand_mult\n",
    "    if return_hypotheses:\n",
    "        return hypotheses\n",
    "    if verbose:\n",
    "        print(len(hypotheses))\n",
    "    result = []\n",
    "    for hypo, cnt in hypotheses.most_common(result_size):\n",
    "        if verbose:\n",
    "            print(cnt, hypo, synset_storage.id2synset[hypo].name())\n",
    "        result.append(hypo)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets: 79689\n",
      "senses 132127\n",
      "forbidden senses are 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00235678674492aac80b0a69874f1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79689.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "numer of ids 79689 long list is 319887\n"
     ]
    }
   ],
   "source": [
    "full_syn_storage = SynsetStorage.from_taxonomy(tx=existing_taxonomy, pos=tx.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79689\n",
      "619\n",
      "6\n",
      "81857\n"
     ]
    }
   ],
   "source": [
    "rel_storage = RelationStorage(forbidden_id=full_syn_storage.forbidden_id)\n",
    "rel_storage.load_relations_from_taxonomy(tx=existing_taxonomy, pos=tx.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedder is the longest part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3a2fb3f5674aee8aded6639d9587fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=319887.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs = np.stack([w2v_embedder(t) for t in tqdm(full_syn_storage.texts_long) ])\n",
    "full_w2v_tree = KDTree(full_w2v_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73ce4b8eda841d4b565e42f42ac7fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=319887.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_pos = np.stack([w2v_embedder_pos(t) for t in tqdm(full_syn_storage.texts_long) ])\n",
    "full_w2v_tree_pos = KDTree(full_w2v_vecs_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_input_text(text):\n",
    "    return text.replace('_', ' ').replace('-', ' ').lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['word_norm'] = train_data['word'].apply(normalize_input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "smpl = train_data.sample(100, random_state=1)\n",
    "smpl_gt = {row.word_norm: row.hypernyms for i, row in smpl.iterrows()}\n",
    "print(*evaluate.get_score(smpl_gt, smpl_gt, k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def w2v_scorer(text1, text2):\n",
    "    return np.dot(w2v_embedder.get_text_vec(text1), w2v_embedder.get_text_vec(text2)) ** 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb37c398d114e349b33736363faaf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.1344155844155844 0.0\n"
     ]
    }
   ],
   "source": [
    "public_test_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        txt, \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree, text2vec=w2v_embedder_pos,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        neighbor_scorer=None, # todo: return it\n",
    "    )  \n",
    "    for txt in tqdm(smpl.word_norm)\n",
    "}\n",
    "print(*evaluate.get_score(smpl_gt, public_test_hypos, k=10))  # 0.1344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a95f72fe4f6479e82aaa904ac0726b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.13384439634439632 0.0\n"
     ]
    }
   ],
   "source": [
    "public_test_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        txt, \n",
    "        index=full_w2v_tree, text2vec=w2v_embedder,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        neighbor_scorer=None, # todo: return it\n",
    "    )  \n",
    "    for txt in tqdm(smpl.word_norm)\n",
    "}\n",
    "print(*evaluate.get_score(smpl_gt, public_test_hypos, k=10))  # 0.1338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca142abd0d774407915533e2339fb14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.14755491422158085 0.0\n"
     ]
    }
   ],
   "source": [
    "public_test_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        txt, \n",
    "        index=full_w2v_tree, text2vec=w2v_embedder,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=0.03, # ok\n",
    "        k=100, \n",
    "        grand_mult=0.6,\n",
    "        neighbor_scorer=None, # todo: return it\n",
    "    )  \n",
    "    for txt in tqdm(smpl.word_norm)\n",
    "}\n",
    "print(*evaluate.get_score(smpl_gt, public_test_hypos, k=10))  # 0.14755491422158085 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7c3a4be66f4e78b78d361502120039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.1706088664421998 0.0\n"
     ]
    }
   ],
   "source": [
    "def w2v_scorer(text1, text2):\n",
    "    return np.dot(w2v_embedder.get_text_vec(text1), w2v_embedder.get_text_vec(text2)) ** 5 \n",
    "\n",
    "public_test_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        txt, \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree, text2vec=w2v_embedder_pos,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        neighbor_scorer=w2v_scorer, # todo: return it\n",
    "    )  \n",
    "    for txt in tqdm(smpl.word_norm)\n",
    "}\n",
    "print(*evaluate.get_score(smpl_gt, public_test_hypos, k=10))  # 0.1706088664421998 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cd158f777343d397eb200e2fb60703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.18505892255892262 0.0\n"
     ]
    }
   ],
   "source": [
    "def w2v_scorer(text1, text2):\n",
    "    return np.dot(w2v_embedder.get_text_vec(text1), w2v_embedder.get_text_vec(text2)) ** 10\n",
    "\n",
    "public_test_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        txt, \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree, text2vec=w2v_embedder_pos,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.8,\n",
    "        neighbor_scorer=w2v_scorer, # todo: return it\n",
    "    )  \n",
    "    for txt in tqdm(smpl.word_norm)\n",
    "}\n",
    "print(*evaluate.get_score(smpl_gt, public_test_hypos, k=10))  # 0.18505892255892262 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e778dec60f15435b8b7e9ab951e86aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.18164582331248996 0.0\n"
     ]
    }
   ],
   "source": [
    "public_test_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        txt, \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree, text2vec=w2v_embedder_pos,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.8,\n",
    "        scorer_pow=10\n",
    "    )  \n",
    "    for txt in tqdm(smpl.word_norm)\n",
    "}\n",
    "print(*evaluate.get_score(smpl_gt, public_test_hypos, k=10))  # 0.18505892255892262 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2submission(word2hypotheses, id2synset):\n",
    "    result_nouns = []\n",
    "    result_hyperonyms = []\n",
    "    for n, h in word2hypotheses.items():\n",
    "        for hypo in h:\n",
    "            result_nouns.append(n)\n",
    "            result_hyperonyms.append(hypo)\n",
    "    result_df = pd.DataFrame({'noun': result_nouns, 'result': result_hyperonyms})\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(public_test_hypos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict2submission(public_test_hypos, id2synset).noun.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'noun'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-c9500b2596f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpublic_test_hypos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'noun'"
     ]
    }
   ],
   "source": [
    "public_test_hypos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit for nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6e30828ab742e6abd069094c6e73a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3129.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_n_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        normalize_input_text(txt), \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree, text2vec=w2v_embedder_pos,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.8,\n",
    "        scorer_pow=5\n",
    "    )   \n",
    "    for txt in tqdm(nouns_data.word)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ready-made</td>\n",
       "      <td>state.n.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ready-made</td>\n",
       "      <td>condiment.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ready-made</td>\n",
       "      <td>artifact.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ready-made</td>\n",
       "      <td>buffer.n.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ready-made</td>\n",
       "      <td>spread.n.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ready-made</td>\n",
       "      <td>memory_device.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ready-made</td>\n",
       "      <td>cost.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ready-made</td>\n",
       "      <td>container.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ready-made</td>\n",
       "      <td>payment.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ready-made</td>\n",
       "      <td>sheet.n.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rancidness</td>\n",
       "      <td>gas.n.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rancidness</td>\n",
       "      <td>compound.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rancidness</td>\n",
       "      <td>fluid.n.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rancidness</td>\n",
       "      <td>organic_compound.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rancidness</td>\n",
       "      <td>smell.n.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          noun                 result\n",
       "0   ready-made             state.n.04\n",
       "1   ready-made         condiment.n.01\n",
       "2   ready-made          artifact.n.01\n",
       "3   ready-made            buffer.n.03\n",
       "4   ready-made            spread.n.05\n",
       "5   ready-made     memory_device.n.01\n",
       "6   ready-made              cost.n.01\n",
       "7   ready-made         container.n.01\n",
       "8   ready-made           payment.n.01\n",
       "9   ready-made             sheet.n.06\n",
       "10  rancidness               gas.n.02\n",
       "11  rancidness          compound.n.01\n",
       "12  rancidness             fluid.n.02\n",
       "13  rancidness  organic_compound.n.01\n",
       "14  rancidness             smell.n.01"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = dict2submission(test_n_hypos, full_syn_storage.id2synset)\n",
    "sub.to_csv('nouns_test_result2.tsv', sep='\\t', encoding='utf-8', header=None, index=None)\n",
    "sub.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map: 0.18766891584878223\n",
      "mrr: 0.20641630316248633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python evaluate.py data/wordnet_data/nouns_en_new.2.0-3.0.tsv nouns_test_result2.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adec55bc60ac42f889c1619508b09dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3129.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "map: 0.18766891584878223\n",
      "mrr: 0.20641630316248633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_n_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        normalize_input_text(txt), \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree_pos, text2vec=w2v_embedder_pos,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        neighbor_scorer=w2v_scorer,\n",
    "        #scorer_pow=5\n",
    "    )   \n",
    "    for txt in tqdm(nouns_data.word)\n",
    "}\n",
    "sub = dict2submission(test_n_hypos, full_syn_storage.id2synset)\n",
    "sub.to_csv('nouns_test_result_tmp.tsv', sep='\\t', encoding='utf-8', header=None, index=None)\n",
    "\n",
    "! python evaluate.py data/wordnet_data/nouns_en_new.2.0-3.0.tsv nouns_test_result_tmp.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map: 0.23411422613998975\n",
      "mrr: 0.2543461468556893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python evaluate.py data/wordnet_data/nouns_en_new.2.0-3.0.tsv nouns_test_result_tmp.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a868221ca15b4c32aa8fbf81b83bf7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=319887.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_n_GW = np.stack([w2v_embedder_gw(t) for t in tqdm(full_syn_storage.texts_long) ])\n",
    "full_w2v_tree_n_GW = KDTree(full_w2v_vecs_n_GW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_w2v_vecs_gw_pos_n = np.stack([w2v_embedder_gw_pos_n(t) for t in tqdm(full_syn_storage_n.texts_long) ])\n",
    "full_w2v_tree_gw_pos_n = KDTree(full_w2v_vecs_gw_pos_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dab30be5f2431586c9bd7adc315df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3129.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "map: 0.23411422613998975\n",
      "mrr: 0.2543461468556893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def w2v_scorer(text1, text2):\n",
    "    return np.dot(w2v_embedder_gw.get_text_vec(text1), w2v_embedder_gw.get_text_vec(text2)) ** 5\n",
    "\n",
    "test_v_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        normalize_input_text(txt), \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree_n_GW, text2vec=w2v_embedder_gw,\n",
    "        #index=full_w2v_tree_gw_pos_v, text2vec=w2v_embedder_gw_pos_v,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        neighbor_scorer=w2v_scorer,\n",
    "        #scorer_pow=5\n",
    "    )   \n",
    "    for txt in tqdm(nouns_data.word)\n",
    "}\n",
    "sub = dict2submission(test_n_hypos, full_syn_storage.id2synset)\n",
    "sub.to_csv('nouns_test_result_tmp.tsv', sep='\\t', encoding='utf-8', header=None, index=None)\n",
    "! python evaluate.py data/wordnet_data/nouns_en_new.2.0-3.0.tsv nouns_test_result_tmp.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit for verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets: 13508\n",
      "senses 11319\n",
      "forbidden senses are 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd487bf4e68e4c9291d2969a9ef0a4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "numer of ids 13508 long list is 51663\n",
      "13285\n",
      "393\n",
      "2\n",
      "12985\n"
     ]
    }
   ],
   "source": [
    "full_syn_storage_v = SynsetStorage.from_taxonomy(tx=existing_taxonomy, pos=tx.VERB)\n",
    "rel_storage_v = RelationStorage(forbidden_id=full_syn_storage_v.forbidden_id)\n",
    "rel_storage_v.load_relations_from_taxonomy(tx=existing_taxonomy, pos=tx.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e16be8fbff453baca8fc7bf51ca315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=51663.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_v = np.stack([w2v_embedder(t) for t in tqdm(full_syn_storage_v.texts_long) ])\n",
    "full_w2v_tree_v = KDTree(full_w2v_vecs_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e69af034204962af8eb0f116b87a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=51663.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_v_pos = np.stack([w2v_embedder_pos_v(t) for t in tqdm(full_syn_storage_v.texts_long) ])\n",
    "full_w2v_tree_v_pos = KDTree(full_w2v_vecs_v_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c18d99bb7db432692315986b856f2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=51663.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_v_GW = np.stack([w2v_embedder_gw(t) for t in tqdm(full_syn_storage_v.texts_long) ])\n",
    "full_w2v_tree_v_GW = KDTree(full_w2v_vecs_v_GW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a527702cf44a8dbc6cbca3e3896e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=51663.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_gw_pos_v = np.stack([w2v_embedder_gw_pos_v(t) for t in tqdm(full_syn_storage_v.texts_long) ])\n",
    "full_w2v_tree_gw_pos_v = KDTree(full_w2v_vecs_gw_pos_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4a5fea214243a1883bd6251890a831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=207.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_v_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        normalize_input_text(txt), \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree_v, text2vec=w2v_embedder,\n",
    "        synset_storage=full_syn_storage_v, rel_storage=rel_storage_v,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        scorer_pow=5\n",
    "    )   \n",
    "    for txt in tqdm(verbs_data.word)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hot-dog</td>\n",
       "      <td>caress.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hot-dog</td>\n",
       "      <td>change.v.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hot-dog</td>\n",
       "      <td>make.v.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hot-dog</td>\n",
       "      <td>excrete.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hot-dog</td>\n",
       "      <td>be.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hot-dog</td>\n",
       "      <td>touch.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hot-dog</td>\n",
       "      <td>treat.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hot-dog</td>\n",
       "      <td>remove.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hot-dog</td>\n",
       "      <td>interact.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hot-dog</td>\n",
       "      <td>appear.v.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>deionize</td>\n",
       "      <td>change.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>deionize</td>\n",
       "      <td>change.v.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>deionize</td>\n",
       "      <td>change_state.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>deionize</td>\n",
       "      <td>change_integrity.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>deionize</td>\n",
       "      <td>remove.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>deionize</td>\n",
       "      <td>solder.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>deionize</td>\n",
       "      <td>wet.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deionize</td>\n",
       "      <td>clean.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>deionize</td>\n",
       "      <td>separate.v.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>deionize</td>\n",
       "      <td>heal.v.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>slough_off</td>\n",
       "      <td>change.v.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>slough_off</td>\n",
       "      <td>make.v.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>slough_off</td>\n",
       "      <td>be.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>slough_off</td>\n",
       "      <td>remove.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>slough_off</td>\n",
       "      <td>shed.v.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          noun                 result\n",
       "0      hot-dog            caress.v.01\n",
       "1      hot-dog            change.v.02\n",
       "2      hot-dog              make.v.03\n",
       "3      hot-dog           excrete.v.01\n",
       "4      hot-dog                be.v.01\n",
       "5      hot-dog             touch.v.01\n",
       "6      hot-dog             treat.v.01\n",
       "7      hot-dog            remove.v.01\n",
       "8      hot-dog          interact.v.01\n",
       "9      hot-dog            appear.v.02\n",
       "10    deionize            change.v.01\n",
       "11    deionize            change.v.02\n",
       "12    deionize      change_state.v.01\n",
       "13    deionize  change_integrity.v.01\n",
       "14    deionize            remove.v.01\n",
       "15    deionize            solder.v.01\n",
       "16    deionize               wet.v.01\n",
       "17    deionize             clean.v.01\n",
       "18    deionize          separate.v.06\n",
       "19    deionize              heal.v.02\n",
       "20  slough_off            change.v.02\n",
       "21  slough_off              make.v.03\n",
       "22  slough_off                be.v.01\n",
       "23  slough_off            remove.v.01\n",
       "24  slough_off              shed.v.01"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = dict2submission(test_v_hypos, full_syn_storage_v.id2synset)\n",
    "sub.to_csv('verbs_test_result2.tsv', sep='\\t', encoding='utf-8', header=None, index=None)\n",
    "sub.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map: 0.153703018340324\n",
      "mrr: 0.15460975409161934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python evaluate.py data/wordnet_data/verbs_en_new.2.0-3.0.tsv verbs_test_result2.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bb92913bd04bbb9ac970d1a04a1881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=207.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "map: 0.17935479891438436\n",
      "mrr: 0.18408586232420424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_v_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        normalize_input_text(txt), \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree_v_pos, text2vec=w2v_embedder_pos,\n",
    "        synset_storage=full_syn_storage_v, rel_storage=rel_storage_v,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        neighbor_scorer=w2v_scorer,\n",
    "        #scorer_pow=5\n",
    "    )   \n",
    "    for txt in tqdm(verbs_data.word)\n",
    "}\n",
    "sub = dict2submission(test_v_hypos, full_syn_storage_v.id2synset)\n",
    "sub.to_csv('verbs_test_result_tmp.tsv', sep='\\t', encoding='utf-8', header=None, index=None)\n",
    "! python evaluate.py data/wordnet_data/verbs_en_new.2.0-3.0.tsv verbs_test_result_tmp.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6db18bc48c4979856f50b4f77694f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=207.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "map: 0.22350316637881418\n",
      "mrr: 0.2322251007484169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def w2v_scorer(text1, text2):\n",
    "    return np.dot(w2v_embedder_gw.get_text_vec(text1), w2v_embedder_gw.get_text_vec(text2)) ** 5\n",
    "\n",
    "test_v_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        normalize_input_text(txt), \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        #index=full_w2v_tree_v_pos, text2vec=w2v_embedder_pos_v,\n",
    "        index=full_w2v_tree_v_GW, text2vec=w2v_embedder_gw,\n",
    "        #index=full_w2v_tree_gw_pos_v, text2vec=w2v_embedder_gw_pos_v,\n",
    "        synset_storage=full_syn_storage_v, rel_storage=rel_storage_v,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        neighbor_scorer=w2v_scorer,\n",
    "        #scorer_pow=5\n",
    "    )   \n",
    "    for txt in tqdm(verbs_data.word)\n",
    "}\n",
    "sub = dict2submission(test_v_hypos, full_syn_storage_v.id2synset)\n",
    "sub.to_csv('verbs_test_result_tmp.tsv', sep='\\t', encoding='utf-8', header=None, index=None)\n",
    "! python evaluate.py data/wordnet_data/verbs_en_new.2.0-3.0.tsv verbs_test_result_tmp.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greco roman wrestling\n"
     ]
    }
   ],
   "source": [
    "txt = normalize_input_text('Greco-Roman_wrestling')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>hypernyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>expense</td>\n",
       "      <td>[devalue.v.02, depreciate.v.02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>eyewitness</td>\n",
       "      <td>[witness.v.01, watch.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fink</td>\n",
       "      <td>[declare.v.04, admit.v.01]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word                        hypernyms\n",
       "11      expense  [devalue.v.02, depreciate.v.02]\n",
       "154  eyewitness       [witness.v.01, watch.v.01]\n",
       "5          fink       [declare.v.04, admit.v.01]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7399135323148253 1 raise.v.02 get_up 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 swing.v.11 get_around 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 take.v.04 get_hold_of 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_off.v.02 get_off 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 cope.v.01 get_by 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 do.v.04 get_along 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 arise.v.03 get_up 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 click.v.07 get_through 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 pull_in.v.03 get_in 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 escape.v.01 get_away 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_well.v.01 get_well 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_even.v.02 get_even 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_in.v.04 get_into 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 senesce.v.01 get_on 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_in_touch.v.01 get_in_touch 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 send.v.06 get_off 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 trip.v.05 get_off 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 annoy.v.01 get_at 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_a_noseful.v.01 get_a_whiff 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_well.v.01 get_over 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 gain.v.05 get_ahead 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 start.v.09 get_going 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_even.v.02 get_back 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_off.v.11 get_off 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 break.v.46 get_out 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 while_away.v.01 get_through 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_off.v.04 get_off 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 catch_on.v.01 get_onto 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 marry.v.01 get_hitched_with 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 roll_in_the_hay.v.01 get_it_on 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 dress_up.v.01 get_up 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_into.v.04 get_into 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 break.v.46 get_around 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_through.v.01 get_through 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_into.v.01 get_into 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 bypass.v.01 get_around 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 master.v.01 get_the_hang 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_along_with.v.01 get_on 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_a_look.v.01 get_a_look 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 settle.v.19 get_back 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 learn.v.02 get_a_line 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 roll_in_the_hay.v.01 get_laid 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_in.v.04 get_in 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_off.v.06 get_off 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 hand_over.v.01 get_in 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 learn.v.02 get_wind 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 line_up.v.02 get_hold 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 arrive.v.02 get_in 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_around_to.v.01 get_around_to 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_down.v.07 get_down 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 write_down.v.01 get_down 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_across.v.01 get_across 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 assemble.v.03 get_together 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_the_best.v.01 get_the_best 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_it.v.02 get_it 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_the_better_of.v.01 get_the_better_of 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 reach.v.04 get_through 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 come_to_grips.v.01 get_to_grips 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 catch_on.v.01 get_it 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 marry.v.01 get_married 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 bring_out.v.07 get_out 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_along_with.v.01 get_along 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 join.v.01 get_together 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 grind_to_a_halt.v.01 get_stuck 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_through.v.03 get_through 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 equalize.v.01 get_even 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 click.v.07 get_across 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 learn.v.02 get_word 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_up.v.02 get_up 0 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_up.v.04 get_up 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_a_noseful.v.01 get_a_noseful 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_to.v.02 get_to 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 hop_out.v.01 get_off 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_along_with.v.01 get_on_with 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 board.v.01 get_on 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 swallow.v.01 get_down 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_around.v.04 get_about 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 cram.v.03 get_up 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 reach.v.07 get_to 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 work_up.v.02 get_up 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 overcome.v.02 get_over 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 catch_on.v.01 get_wise 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_along_with.v.01 get_along_with 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_off.v.01 get_off 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_the_picture.v.01 get_the_picture 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 collaborate.v.01 get_together 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 hop_on.v.01 get_on 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 exit.v.01 get_out 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 boom.v.05 get_ahead 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 draw.v.05 get_out 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_at.v.02 get_at 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 escape.v.06 get_away 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_on.v.05 get_on 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_stranded.v.01 get_stranded 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 traverse.v.01 get_over 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 pull_out.v.01 get_out 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 access.v.02 get_at 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 lower.v.01 get_down 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 get_worse.v.01 get_worse 1 0.6669221426877154 0.007510991575217954\n",
      "0.7399135323148253 1 annoy.v.01 get_to 1 0.6669221426877154 0.007510991575217954\n",
      "79\n",
      "0.030043966300871815 relate.v.05 relate.v.05\n",
      "0.030043966300871812 interact.v.01 interact.v.01\n",
      "0.02253297472565386 move.v.02 move.v.02\n",
      "0.02253297472565386 change_state.v.01 change_state.v.01\n",
      "0.02253297472565386 get_the_picture.v.01 get_the_picture.v.01\n",
      "0.02253297472565386 move.v.03 move.v.03\n",
      "0.018777478938044885 act.v.01 act.v.01\n",
      "0.018777478938044885 change.v.01 change.v.01\n",
      "0.018777478938044885 understand.v.01 understand.v.01\n",
      "0.015021983150435908 better.v.03 better.v.03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['relate.v.05',\n",
       " 'interact.v.01',\n",
       " 'move.v.02',\n",
       " 'change_state.v.01',\n",
       " 'get_the_picture.v.01',\n",
       " 'move.v.03',\n",
       " 'act.v.01',\n",
       " 'change.v.01',\n",
       " 'understand.v.01',\n",
       " 'better.v.03']"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_knn(\n",
    "        'google', \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree_v_pos, text2vec=w2v_embedder_pos,\n",
    "        synset_storage=full_syn_storage_v, rel_storage=rel_storage_v,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        neighbor_scorer=w2v_scorer,\n",
    "        #scorer_pow=5\n",
    "        verbose=True,\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
